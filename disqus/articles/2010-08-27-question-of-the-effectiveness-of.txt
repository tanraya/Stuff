title: "К вопросу об эффективности"

<body>
<p>Привет.
Сегодня моему коллеге понадобилось внести количество населения в городах России в его табличку cities в БД. Он, конечно, начал писать парсер Википедии, дабы добыть нужную информацию из страниц городов на сайте. То есть, он хотел обращаться к Википедии примерно так:
</p>
<pre>http://ru.wikipedia.org/wiki/&lt;название города&gt;</pre>
Он просто обращался по этому адресу при помощи php-функции file_get_contents(), но ему возвращалась 403 ошибка. Я обратился телнетом к странице и узнал, что Википедия требует присутствие заголовка User-Agent в HTTP-запросе.
Показал коллеге как юзать telnet и формировать HTTP-запрос =) Этим убедил его в том, что ему нужно юзать cURL. И он принялся писать свой мега-парсер.

Но лично я получил требуемый список городов за 5 минут. Как я это сделал? Я просто использовал более подходящие инструменты и шел легким путем.

Для начала я нашел HTML-табличку со списком городов и населением в них. Потом я скопировал эту табличку при помощи firebug в отдельный HTML-файл, подключил туда JQuery с Google CDN, и написал абсолютно тупой джаваскрипт, который обходил табличку и забирал оттуда нужные данные, а потом писал их в лог при помощи console.log(). Знаю, что можно было выполнить свой JS прямо на странице Википедии, но не знаю почему так не сделал :). Дальше осталось только передать эти данные коллеге.
Я потратил на это всего минут 5, мой коллега потратил бы на это половину рабочего дня, т.к. парсинг HTML-страниц в PHP это дело непростое, там нет <a href="http://mechanize.rubyforge.org/mechanize/">Mechanize</a>, к тому же Википедия может забанить по IP при парсинге кучи страниц :).

Вот как-то так, стоит всегда использовать подходящие инструменты и немного задействовать программерскую смекалку.</body>
